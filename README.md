# Data_Analysis_Portfolio

Портфолио по анализу данных 
с описанием постановки задач 
и метода решения

СОДЕРЖАНИЕ

1	Импорт данных
2	Данные из нескольких источников
3	Выделение данных
4	Предсказание на 2020 год
5	Получение данных по API
6	Получение котировок акций
7	Парсинг интернет-магазина
8	Загрузка результатов в БД
9	Тип визуализации данных
10	Результаты марафона
11	Скользящие средние на биржевых графиках
12	Объекты культурного наследия России
13	Сборка PDF документа
14	Геральдические символы Москвы
15	Многостраничный отчёт
16	Автоматические отчёты


Задание 1. Импорт данных
Возьмите данные по вызовам пожарных служб в Москве за 2015-2019 годы:
https://video.ittensive.com/python-advanced/data-5283-2019-10-04.utf.csv
Получите из них фрейм данных (таблицу значений). По этому фрейму вычислите среднее значение вызовов пожарных машин в месяц в одном округе Москвы, округлив до целых
Примечание: найдите среднее значение вызовов, без учёта года.

Решение
Для расчёта среднего значения данных из csv-файла, нужно сначала загрузить этот файл, используя библиотеку «Pandas», затем нужно посмотреть на столбцы, которые есть в этом файле, найти нужный столбец и посчитать для него среднее значение. 
Сначала импортируется библиотека «Pandas», читается файл с помощью функции «.read_csv» с указанием адреса, где расположен файл, выводятся заголовки. 
В этом файле нестандартный разделитель, поэтому дополнительный параметр, который потребуется это «delimiter», который нужно назначить, в данном случае «;». Это позволит получить данные в нормальном виде, которые будут распознаны и получены «Pandas».
Из выведенных данных наиболее подходит под число вызовов столбец «Calls».
Нужно получить среднее. Для этого необходимо обратиться к столбцу данных и вывести среднее значение, округлив его, используя функцию «round».
Код и пояснения к нему можно найти в файле: «Задание 1. Импорт данных.ipynb». 


Задание 2. Данные из нескольких источников
Получите данные по безработице в Москве:
https://video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv
Объедините эти данные индексами (Месяц/Год) с данными из предыдущего задания (вызовы пожарных) для Центрального административного округа:
https://video.ittensive.com/python-advanced/data-5283-2019-10-04.utf.csv
Найдите значение поля «UnemployedMen» в том месяце, когда было меньше всего вызовов в Центральном административном округе.

Решение
Для решения необходимо импортировать необходимые библиотеки и загрузить данные по безработице. Для этих данных выставим индекс по году и месяцу. Необходимо будет привести разные колонки к одному индексу.
При загрузке второго набора данных и выводе данных можно увидеть названия столбцов: есть административный округ, год, месяц. Нужно выбрать все значения по Центральному административному округу, для этого необходимо назначить индекс по трём полям: округ, год, месяц. Год и месяц будут использоваться для объединения с первым набором данных, округ будет использоваться для того, чтобы выбрать по нему значения.
После того как назначен индекс можно выбрать по нему все значения, которые нужны, используя метод «loc» и вывести название индекса. 
Данные из второго набора нужно подготовить к объединению. Можно при загрузке данных указать нужные имена через свойство «names», отбросить те названия колонок, в которых есть: «ID», «GlobalID», «Calls» и назначить свои колонки, используя вместо названия «Month» название «Period» и в этом случае можно по индексам, по множественному индексу из года и месяца их объединить.
После этого происходит объединение двух наборов данных по индексам. Необходимо, чтобы все индексы по наборам данных были идентичны.
Произведено соотношение «Calls» и «Unemploymen». Теперь можно отбросить индекс, так как он задачу решил и назначить новый индекс по числу вызовов. Для того чтобы найти минимальное значение происходит сортировка по этому индексу.
Выводится значение серии «UnemployedMen» и делается для него срез, выводится первое значение – это и есть вывод результата.          
Код и пояснения к нему можно найти в файле: «Задание 2. Данные из нескольких источников.ipynb». 
 

Задание 3. Выделение данных
Получите данные по безработице в Москве:
https://video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv
Найдите, с какого года процент людей с ограниченными возможностями (UnemployedDisabled) среди всех безработных (UnemployedTotal) стал меньше 2% в Москве.

Решение
Загружаю необходимые библиотеки и импортирую набор данных. 
Чтобы выполнить фильтрацию завожу отдельный столбец, в который вписываю процент всех людей с ограниченными возможностями. Для того чтобы найти, что в эту колонку вписать применяю изменения построчно, используя параметр «axis=1».
Теперь нужно посчитать какие столбцы с какими нужно сложить. Нужно 6 столбец разделить на 7 столбец. Применяю фильтр по значению этой суммы. Фильтрация в данном случае сделана через дополнительный столбец с использованием лямбда функции и метода «apply».
После фильтрации данных осталось немного значений, видим год, где процент меньше 2, для вывода применяю сортировку и добавляю индекс для этого. «Data» – отсортированное значение по индексу. Используя срезы, вывожу первое значение по индексу. Это и есть ответ на задание.
Код и пояснения к нему можно найти в файле: «Задание 3. Выделение данных.ipynb». 


Задание 4. Предсказание на 2020 год
Возьмите данные по безработице в городе Москва:
video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv
Сгруппируйте данные по годам, и, если в году меньше 6 значений, отбросьте эти годы.
Постройте модель линейной регрессии по годам среднего значения отношения UnemployedDisabled к UnemployedTotal (процента людей с ограниченными возможностями) за месяц и ответьте, какое ожидается значение процента безработных инвалидов в 2020 году при сохранении текущей политики города Москвы? Ответ округлите до сотых. Например, 2,32.
Какое ожидается значение в 2020 году при сохранении текущей политики города Москвы?

Решение
Подключаю все необходимые библиотеки и загружаю набор данных. Получаю данные через метод «.read_csv» и использую разделитель по «;».
Чтобы корректно обработать данные нужно для каждой строки данных вычислить значение некоторого дополнительного столбца, таким образом получится для каждой строки новая серия данных из значений процента, которую далее необходимо будет предсказать.
Фильтрую данные, для этого выполняю группировку по году, используя метод «.groupby» и после него использую метод фильтрации – лямда функцию, которая принимает группу данных, которую должны применить к группе данных групповой операции, посчитаю число строк, которые вошли в эту группу данных, количество должно быть больше 5. Все годы, где меньше 6 записей отбрасываю. Фильтр возвращает не группу, а сами данные, поэтому группы данных нужно заново сгруппировать по году и взять для них среднее значение, после этого можно посмотреть на данные, к которым нужно применить линейную регрессию.
Привожу индексы группы данных к массиву и потом изменяю их форму на то, чтобы это был двумерный массив методом «reshape». Использую значения столбца «UDP» для каждой группы данных, теперь можно подгрузить модель линейной регрессии и её инициализировать значениями через метод «fit».
Вывожу предсказанное значение, которое приводится к форме двумерного массива. Привожу значение к массиву, а затем двумерному массиву, состоящему из одной ячейки –  «.reshape(1, 1)».
После этого запускаю линейную регрессию и предсказание значения. 
Код и пояснения к нему можно найти в файле: «Задание 4. Предсказание на 2020 год.ipynb». 

             
Задание 5. Получение данных по API
Изучите API Геокодера Яндекса
tech.yandex.ru/maps/geocoder/doc/desc/concepts/input_params-docpage/
и получите ключ API для него в кабинете разработчика.
Выполните запрос к API и узнайте долготу точки на карте (Point) для города Самара.
В качестве запасного варианта можно использовать этот ключ - 3f355b88-81e9-4bbf-a0a4-eb687fdea256 - он только для выполнения этого задания!
Какая долгота у точки на карте для Самары?

Решение
Нужно перейти на страницу геокодера и получить ключ API, будет всплывающее окно справа внизу и при наличии аккаунта Яндекс, можно получить ключ API для работы с геокодером.
После получения ключа API нужно изучить формат API для того, чтобы отправить запрос. В квадратных скобках необязательные параметры и без скобок обязательные параметры, которые должны быть использованы. 
Необходимо понять, что передать в Geocode – можно передать либо адрес, либо географические координаты. Достаточно передать «Самара» для получения её координат.
Заполняю ключ API и geocode.
При просмотре есть параметр «format», ответ в данном случае будет легче разобрать «json», ввожу в запрос. Также будет полезно взять «result» - максимальное количество возвращаемых объектов, можно ограничить одним объектом.
Подключаю библиотеки, отправляю get-запрос к API. Geo-объект – это “Json.loads(r.content)” – ответ разбирается в «Json». Получилось много текста, сложная структура “Json”.
В ответе можно увидеть, что есть “point”, где есть координаты: первое искомая широта точки, которую необходимо было найти. Необходимо найти точку центра и вычленить из geo.
Начинаю с того, что последовательно сокращаю объект до тех пор пока не получу сами координаты. Есть «metaDataProperty», «featureMember» - смотрю, что это массив, в квадратных скобках, нужно взять первый объект массива и затем перейти к геообъекту, вывести «point» и затем «pos» – можно разделить по пробелу и вывести первую координату.  
Код и пояснения к нему можно найти в файле: «Задание 5. Получение данных по API.ipynb». 
              

Задание 6. Получение котировок акций
Получите данные по котировкам акций со страницы:
mfd.ru/marketdata/?id=5&group=16&mode=3&sortHeader=name&sortOrder=1&selectedDate=01.11.2019 и найдите по какому тикеру был максимальный рост числа сделок (в процентах) за 1 ноября 2019 года.

Решение
Подключаю библиотеки, загружаю данные.
Нужно найти таблицу с котировками и посмотреть на процент сделок, чтобы он был максимальным.
Получаю html-код с веб-страницы. Для нахождения таблицы ищу по тегу таблицы «table».
Идут различные данные по тикерам. У нужной таблицы маркер «id=”marketdataList”», таргетироваться по id лучше, но может не всегда сработать (table = html.find).
Перехожу к разбору таблицы.
Нужно найти все теги «tr» внутри таблицы, перебирая все теги в строку генератором, добавляю получение текста из ячеек «td.get_text()», а сами «td» находятся в html-коде, который был найден на строку при разборе тега «tr» внутри тега «table». 
Внутри тега «table» ищу теги «tr», все html-коды соответствующие им и затем в каждом из этих html-кодов ищу, выбирая все ячейки и делая небольшой список, также в переменную «tr» перезаписываем этот список для того чтобы отметить, если будут, строки нулевой длины, (например, есть «th» внутри строки, есть таблица, есть  тег «tr» внутри его идёт тег «th», table header не нужен в данном случае, но когда происходит перебор всех строк, в header может окажется, что в каких-то строках нет данных, достаточно их отбросить, если длина (размер) этого списка больше 0, то к строкам, то добавляю разобранную строку из таблицы).
Получаю таблицу «Market Data List» и выбираю из неё все значения по котировкам акций. Есть много значений, и они соответствуют тому, что нужно получить. Также добавляю аргумент strip=true, чтобы выбросить из значений, значения по росту в процентах – это будет рост сделок, он у нас выходит «чистым», но для гарантии, что все остальные данные «хорошие», много пробелов и переводов строк нужно вырезать, в функции «get_text» поможет параметр “strip=true”. После этого получились более очищенные данные.
Выгружаю в DataFrame весь список списков строк, есть столбцы, серии данных вызываются соответствующим образом, можно их посмотреть на странице котировки: тикер, дата, сделки, число сделок, изменение числа сделок и процент изменения числа сделок, закрытие предыдущего периода, открытие предыдущего периода, минимальная цена, максимальная цена и средняя цена, объём в количествах, объём в рублях и количестве сделок – все эти названия колонок добавляю.
Нужно удалить из набора данных те сделки, которые отсутствуют, чтобы не влияли никак на сортировку процента по сделкам, иначе они могут сбить ключи, так как неверно можно интерпретировать значения – отсортирую, а затем пойдут пустые ключи.
Преобразую столбец процентов. Убираю из столбца дополнительные символы. Если внимательно посмотреть, то черта не является дефисом, а минусом, поэтому его нужно будет заменить, чтобы отрицательные значения разобрались хорошо и удалить процент из колонки со сделками можно привести всю колонку, используя «astype» привести всё к «float».
После просмотра, что получилось, нужно выставить индекс по этим данным, отсортировать данные по индексу и вывести первое значение, которое будет в отсортированном массиве – это и даст ответ: тикер с максимальным ростом на сегодня. Необходимо отсортировать в порядке убывания, нужно взять самый максимальный рост за первое ноября. В этом случае используется параметр «ascending=false» по функции «sort_index». Выведя данные получим отсортированный набор данных – первая строка даёт ответ. Нужно вывести тикер, беру «data(“Тикер”)», отсортировываю его по набору данных, вывожу его первое значение.
Код и пояснения к нему можно найти в файле: «Задание 6. Получение котировок акций.ipynb». 


Задание 7. Парсинг интернет-магазина
Используя парсинг данных с маркетплейса beru.ru, найдите, на сколько литров отличается общий объем холодильников «Саратов 263» и «Саратов 452» ?
Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу: video.ittensive.com/data/018-python-advanced/beru.ru/
На сколько литров отличается общий объем холодильников?

Решение
Необходимо зайти на сайт и ввести в строку поиска «Саратов», выбираю холодильники из поиска «Саратов 263» и «Саратов 452», нужно получить ссылки на эти страницы и получить с этих страниц данные.
Подключаю необходимые библиотеки. Через библиотеку “Request” делаю запрос. Задаю «User-Agent» для соблюдения этики парсинга. В запросе передаю заголовки «header=headers». Получаю html-код, используя библиотеку «BeautifulSoup».
В появившемся ответе нужно найти “Саратов 263”, блоки с товарами, во всех блоках с товарами найти ссылки. Блок с товаром заключён в одну большую ссылку, можно получить все ссылки и найти ссылки и в этих ссылках найти «Саратов 263» в русском варианте написания, как по заданию.
Нахожу все необходимые ссылки в html-коде, позиционируюсь по ним, это все ссылки определённого класса, в этих ссылках ищу определённый текст, перебираю ссылки.
По умолчанию ссылки – это объект «BeautifulSoup», теги нужно привести к строке и в строке уже найти текст «Саратов 263» и, если текст найден, то соответствующая ссылка будет равна атрибуту «href». То же самое для «Саратов 452» 
Если ссылки успешно получены, смотрю их контент. Получаю один и тот же контент с 2-х страниц, поэтому можно создать функцию получение контента со страницы во избежание большего количества кода.
Рассматриваю ссылку «link_263», вывожу информацию, чтобы посмотреть, на что нужно спозиционироваться, где найти объём, который нужен. Общий объём находится в классе «_112Tad-7AP»
Получены все товары по «Саратов» и по ним ссылки, ищу по ссылке объём.
Нужно выделить число из объёма. Можно сделать генератор (цикл), проверить все символы в строке, получаемой из этого тега и если символы цифры, то добавить в строку, на выходе функции получить список символов, состоящий уже из цифр, которые были найдены в html. Привожу строку к целому числу, на выходе получаю финальный объём.
Для каждой ссылки необходимо вывести разницу между ними, взять по модулю, что будет ответом на вопрос задания.                           
Код и пояснения к нему можно найти в файле: «Задание 7. Парсинг интернет-магазина.ipynb». 


Задание 8. Загрузка результатов в БД
Соберите данные о моделях холодильников Саратов с маркетплейса beru.ru: 
URL, название, цена, размеры, общий объем, объем холодильной камеры.
Создайте соответствующие таблицы в SQLite базе данных и загрузите полученные данные в таблицу beru_goods.
Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу:
video.ittensive.com/data/018-python-advanced/beru.ru/
Какая ширина у холодильника Саратов 264 в см?
 
Решение
Подключаю все необходимые библиотеки. Необходимо на сайте взять некоторую веб-страницу за основу. В поиск ввожу «Саратов», выбираю «Холодильники Саратов».
Прежде чем собирать данные по веб-странице, взятой за основу, открою отдельно URL по какому-либо конкретному холодильнику, информация на веб-странице нужна будет для получения и загрузки в базу данных параметров: коротко о товаре (ШхВхГ).
Необходимо посмотреть параметры, определить какие нужны поля, создать таблицу под эти поля.
Начинаю сбор данных с веб-страницы, взятой за основу. Для того, чтобы корректно распарсить ввожу дополнительную функцию, которая будет все необходимые данные собирать с отдельной страницы. 
Нужно посмотреть какие классы отыскать в html-коде, ищу данные по ссылке. Далее перехожу к нахождению ссылок на исходной странице.
Есть тег «h1», в котором содержится «title» – заголовок, название холодильника, поэтому поиск будет осуществляться по обозначенным тегам, «h1» должен быть с классом, так как тегов «h1» может быть несколько. У «h1» только один класс, позиционируюсь по классу, беру текст у тега «title», дальше ищу цену, нужно найти, где есть строка (10728), здесь «json» массивы в html-коде, далее есть тег «span» с ценой, также есть валюта. Позиционирусь по «data-tid», беру первый тег «span» с этим классом.
Цена найдена, пока она в виде строки, её нужно будет привести к числу. Ищу все параметры, которые есть. Есть характерная строка с параметрами «ШхВхГ», которую ищу, теперь нужно по тегу «span» спозиционироваться – «ШхВхГ», посмотреть сколько таких «span», их 4, и они охватывают все параметры «ШхВхГ», общий объём, объём холодильной камеры.
Ввожу переменные под ширину = 0, глубину = 0, высоту = 0, объём холодильной камеры = 0, объём морозильной камеры = 0.
Если эти параметры не будут найдены на веб-странице, то необходимо, чтобы они точно были определены, например, для какого-то холодильника не указан объём морозильной камеры. 
Перебираю теги, вычленяю из тегов эти параметры. Беру текст из тега, анализирую текст. Если в теге присутствует строка «ШхВхГ», то распарсиваю эту строку. Строку разделяю по «:», нужно взять вторую часть из «:» с индексом 1, разделяю строку по (:), теперь эту часть дополнительно отделяю по разделителю «х» для получения значений, которые нужно затем будет привести к числам, типу данных «float»: ширина – первое значение, глубина –  второе значение и высота – третье значение.
Если в теге присутствует, например, общий объём, то значение «tag.find» должно быть больше -1, вычленяющие из тега число. Для этого ввожу функцию-помощник: «find_number», которая будет возвращать приведение к целому числу по нужной строке, проверяя, что все символы в строке являются числами, последовательно объединяя их. 
Из строки текста извлекаются только цифры.
Вывожу «return» – список значений переменных для заполнения базы данных.
Для цены нужно найти тег «span» по классу «data-tid» атрибут должен равняться данному значению «c3eaad93», тогда функция вернёт все правильные значения. Так как значения разделяю по «х», то будут попадать сантиметры, чтобы их удалить, можно разделить по пробелу и взять только первый элемент.
Все элементы распарсились: есть ссылка, название, цена, все размеры, которые требуются.
Для разбора основной веб-страницы запускаю созданную вспомогательную функцию, которая находит к холодильнику нужные данные.
По выводу данных - все холодильники «Саратов» были с ссылками «Саратов», все блоки были в ссылках заключённых в «grid-snippet». Нужно найти все такие ссылки, перебираю ссылки по одной, ввожу массив «data», чтобы приступить к работе с БД. В ссылках нужно получить атрибут «href», если он есть и если есть текст ссылки содержат «Саратов», тогда в данные добавляется результат работы функции «find_data». В этот момент происходит перебор по страницам сайта для получения данных. После вывода информации нужно посмотреть какие данные можно добавить в базу.
Создаю таблицу, выполняю запрос на создание таблицы, передаю все требуемые поля в SQL-запросе. После выполнения запроса нужно записать результат операции. После того как все данные получены, передаю их одним запросом и вставляю данные в базу. Добавляю запрос, передаю в качестве значений список списков, нужно вставить 8 значений, произвожу запись результата операции.
Была создана таблица в БД и добавилась в неё информация. Закрываю соединение с БД.     
Код и пояснения к нему можно найти в файле: «Задание 8. Загрузка результатов в БД.ipynb».  


Задание 9. Тип визуализации данных
Загрузите данные по ЕГЭ за последние годы
https://video.ittensive.com/python-advanced/data-9722-2019-10-14.utf.csv
выберите данные за 2018-2019 учебный год.
Выберите тип диаграммы для отображения результатов по административному округу Москвы, постройте выбранную диаграмму для количества школьников, написавших ЕГЭ на 220 баллов и выше.
Выберите тип диаграммы и постройте её для районов Северо-Западного административного округа Москвы для количества школьников, написавших ЕГЭ на 220 баллов и выше.
Сколько школьников в Строгино, написавших ЕГЭ на 220 баллов и выше?

Решение
Подключаю необходимые библиотеки и загружаю данные. 
Преобразую данные по округу и району, чтобы устранить дубликаты и сделать подписи короче, в частности удаляю из района район, из округа удаляю все слова, кроме первого. Дополнительно назначаю округ и район как категорию, чтобы группировка происходила быстрее, выставляю индекс по году и фильтрую по индексу, чтобы получить данные только за 2018-2019 учебные года, после этого сбрасываю индекс. Снова необходимо посмотреть на данные, теперь остались данные только 2018-2019 учебные года. 
Так как данных по округам и районам достаточно много, и вместе они образуют совокупность, то лучше использовать круговую диаграмму. 
Необходимо вывести две круговые диаграммы в одну строку. Сначала создаю распределение отличников по ЕГЭ по округам, добавляю заголовок к данным «ЕГЭ в Москве», отсекаю данные только отличников, группирую по округам, вывожу сумму на круговой диаграмме. Все преобразования с данными выношу сразу в переменную «data_adm», она потребуется для дальнейшей фильтрации данных по районам, поэтому оставляю в ней только индекс.
Для второй диаграммы выбираю все данные в Северо-Западном округе, добавляю область для вывода данных и в эту область вывожу заголовок «ЕГЭ в СЗАО». После этого отсекаю только отличников и создаю круговую диаграмму по районам. Для выведения подписи к районам нужно вычислить общее число отличников по районам. Вычислив общее число отличников, и, рассчитав точное значение, сначала обязательно нужно округлить значения по району и только потом привести к целому числу, чтобы не потерялась точность из-за двойного выведения значений.                           
Код и пояснения к нему можно найти в файле: «Задание 9. Тип визуализации данных.ipynb». 


Задание 10. Результаты марафона
Загрузите данные по итогам марафона
https://video.ittensive.com/python-advanced/marathon-data.csv
Приведите время половины и полной дистанции к секундам. Найдите, данные каких серии данных коррелируют (используя диаграмму pairplot в Seaborn).
Найдите коэффициент корреляции этих серий данных, используя scipy.stats.pearsonr.
Постройте график jointplot для коррелирующих данных.
Чему равен коэффициент корреляции максимально зависимых серий данных?

Решение
Подключаю необходимые библиотеки, загружаю данные через «.read_csv». 
В данных есть три числовых поля, также есть номинативная серия данных – это «пол», её можно использовать, например, как категорию при рассмотрении корреляции числовых данных. Нужно построить парный график по всем числовым сериям данных и посмотреть их взаимную корреляцию. Столбцы «split», «final» не представлены в виде чисел – это значение времени в формате, поэтому требуется перевести их из этого формата в какое-либо число, например, к общему числу секунд в часах, минутах и секундах. Для этого вывожу специальную функцию для конвертации времени. Функция будет принимать на вход строку, описывающую время и возвращать через генератор, перебирающий пары значений, непосредственно число и его вес в секундах (3600 сек. для часов, 60 сек. для минут и 1 для секунд), потом необходимо взять сумму этих произведений чисел и их весов и вернуть из функции. Значения часов, минут и секунд можно получить, разделив исходную строку времени по двоеточию. Применяю функцию к обеим сериям данных и строю парный график, чтобы посмотреть какие данные с какими коррелируются. Для групп или категорий использую пол спортсменов. 
Из-за большого числа значений графики выглядят достаточно гладко, возможно, распределение возраста по мужчинам и женщинам, мужчины в среднем старше, однако, корреляции у возраста с промежуточными и финальными результатами нет, есть только корреляция промежуточного и финального результата. Характерная линия на парном графике, если спортсмен быстро пробежал полумарафон, то и весь марафон он пробежит быстро.  
Строю корреляционный график для двух этих переменных и нахожу коэффициент корреляции Пирсона, подключаю «skipy.stats» и создаю график «jointplot». 
Дополнительно вывожу коэффициент Пирсона по двум сериям данных отдельно. На финальном графике видны изолинии для коррелирующих серий. Итоговое значение коэффициента корреляции – 0,96.    
Код и пояснения к нему можно найти в файле: «Задание 10. Результаты марафона.ipynb». 


Задание 11. Скользящие средние на биржевых графиках
Используя данные индекса РТС за последние годы
https://video.ittensive.com/python-advanced/rts-index.csv
постройте отдельные графики закрытия (Close) индекса по дням за 2017, 2018, 2019 годы в единой оси X. Добавьте на график экспоненциальное среднее за 20 дней для значения Max за 2017 год.
Найдите последнюю дату, когда экспоненциальное среднее максимального дневного значения (Max) в 2017 году было больше, чем соответствующее значение Close в 2019 году (это последнее пересечение графика за 2019 год и графика для среднего за 2017 год).
После какого дня 2019 года индекс РТС по закрытию окончательно превысил максимальные показатели за аналогичный день 2017 года?

Решение
Для решения задачи по построению индекса РТС можно нанести все графики на различные области друг под другом и найти значения пересечения данных за 2017-2019 годы аналитически.
Привожу все данные к дням года, чтобы отобразить их в единой оси х, заполняю промежутки и затем наношу все данные на один график и уже графически можно посмотреть на решение задачи.
Подключаю все библиотеки. Загружаю данные. Нужно привести строку к дате-времени и развернуть данные в обратном порядке, они организованы сначала самые свежие, а в конце самые старые, нужно, чтобы вначале шли самые старые данные, а затем самые свежие. Создаю дату из строки, дополнительно указываю параметр «dayfirst», чтобы все даты корректно отобразились русского формата в английский (латинский).
Переиндексирую все данные и заполняю пустые даты предыдущими значениями – это потребуется для сравнения серии данных по годам, чтобы в каждый день года было хотя бы одно значение. Добавляю ещё одну серию данных день года для подписи по осям х и назначаю название индекса, которое потерялось при переиндексации данных. Сортирую по индексу, чтобы развернуть данные в правильном хронологическом порядке.
Наношу данные на график. Для данных за 2019 год, создаю отдельный набор, чтобы впоследствии искать искомую дату превышения, как пересечение его с набором за 2017 год. Аналогичным образом создаю набор данных для 2017 года для него беру экспоненциальное среднее со сдвигом 20 от значения «max». Значения экспоненциального среднего будет отличаться от исходного массива данных без заполненных пропусков по дням, но в контексте решения текущей задачи это несущественно, так как будет найдено реальное превышение значений над другим реальным значением. 
Наношу обычные данные за 2017 год в виде фоновой области и за 2018 год в виде ещё одной линии. Добавляю легенду с подписями данных. На графике примерно видна искомая дата – это февраль-март 2019 года, когда значения индекса РТС окончательно превзошли максимум за 2017 год. Теперь можно найти точную дату.
Сначала фильтрую данные за 2019 год, когда закрытие дня было больше максимума аналогичного дня за 2017 год, выставляю индекс по дате, сортирую по индексу и вывожу первое значение, то есть последнюю дату, когда дневной максимум в 2017 году был больше закрытия в соответствующем дне 2019 года. Место назначения индекса сортировки по нему можно использовать «sort values».
Ответ: после 19 февраля 2019 года значения индекса РТС окончательно превзошло максимум дневных торгов в 2017 году.                   
Код и пояснения к нему можно найти в файле: «Задание 11. Скользящие средние на биржевых графиках.ipynb». 


Задание 12. Объекты культурного наследия России
Изучите набор данных по объектам культурного наследия России (в виде gz-архива):
https://video.ittensive.com/python-advanced/data-44-structure-4.csv.gz
и постройте фоновую картограмму по количеству объектов в каждом регионе России, используя гео-данные
https://video.ittensive.com/python-advanced/russia.json
Выведите для каждого региона количество объектов в нём.
Посчитайте число объектов культурного наследия в Татарстане.
Чему равно число объектов культурного наследия в Алтайском крае?

Решение
Так как имеется большой объём данных, оба файла больше 20 Мб, по сети данные будут загружаться долго, поэтому их можно загрузить локально и работать с их локальными копиями. Также из-за большого объёма данные могут не поместиться в оперативной памяти компьютера.
Для загрузки набора данных можно их ограничить сериями данных объектом и районом, чтобы сэкономить оперативную память и вычислительные ресурсы. В этих двух наборах данных разные названия регионов. В наборе данных с описанием объектов культурного наследия они одни, а в географическом наборе другие. Потребуется преобразовать название региона, привести  к единому виду, к единому формату, чтобы данные совместились без потерь.
Подключаю необходимые библиотеки и загружаю основной набор данных, привожу регион к верхнему регистру, важно чтобы регистр региона в обоих наборах данных был одинаковым, чтобы можно было их совместить по региону. После группирую по региону и считаю число объектов по каждому региону. 
Загружаю геоданные. Привожу данные к проекции Меркатора и объединяю наборы данных – основного и гео. Объединение выполняется по региону, за исходный набор данных беру гео, в него добавляются данные по объектам, чтобы вывести эти данные на картограмме. Чтобы найти какие названия расходятся вывожу все строки, где поле объект, то есть число объектов в регионе «NULL». Объединение по регионам корректно не прошло, регионов не было в исходном наборе, таких регионов получилось 6. Нужно заменить их названия в гео наборе данных на нужные или можно заменить в исходном наборе на те названия, которые существуют в гео наборе - унифицировать разные названия одного и того же региона, привести эти названия к какому-то одному в обоих наборах данных.
Для этого можно вывести индекс по основному набору данных, он содержит все названия регионов и найти все расхождения, которые уже выведены в наборе нулевых значений в результирующем объединённом наборе данных. После этого применяю “replace” к данным и заменяю одно название региона на другое: для Ханты-Мансийского автономного округа, затем для Адыгеи, затем для Мари-Эл, Татарстана, Чувашии и для Северной Осетии.
После замены всех регионов снова объединяю данные и теперь расхождений нет. Все регионы получили корректное число объектов культурного наследия.
Для отрисовки фоновой картограммы создаю холст и область отрисовки, на эту область наношу картограмму, использую число объектов культурного наследия в регионе для цветовой градации. Дополнительно наношу аннотацию в виде числа объектов в каждом регионе, перебрав все строки в результирующем наборе данных. Число объектов размещаю в центроиде области, отвечающего за конкретный регион. 
Если сейчас вывести картограмму, то она развернётся по всем долготам из-за Чукотского АО и будет достаточно мелкой. Можно обрезать Чукотский АО и всё, что западнее Калининградской области, для этого задаю лимиты по оси х через «set_xlim», координаты указаны в десятках миллионов: 1е7, поэтому нужен диапазон от 2-х миллионов до 20-ти миллионов от 2е6 до 2е7. 
В завершении вывожу значение числа объектов для Алтайского края из объединённого набора данных.
Код и пояснения к нему можно найти в файле: «Задание 12. Объекты культурного наследия России.ipynb». 


Задание 13. Сборка PDF документа
Используя данные по посещаемости библиотек в районах Москвы
https://video.ittensive.com/python-advanced/data-7361-2019-11-28.utf.json
постройте круговую диаграмму суммарной посещаемости (NumOfVisitors) 20 наиболее популярных районов Москвы.
Создайте PDF отчёт, используя файл 
https://video.ittensive.com/python-advanced/title.pdf как первую страницу. 
На второй странице выведите итоговую диаграмму, самый популярный район Москвы и число посетителей библиотек в нём.
Сколько посетителей библиотек в районе Москвы с самой большой суммарной посещаемостью библиотек?

Решение
Подключаю все необходимые библиотеки и загружаю данные.
Имеются данные в формате «Json» необходимо их сначала загрузить, потом передать во фрейм данных – результат разбора Json-файла, также заполняю нулями отсутствующие значения. Название района скрыто в поле «District», которое само по себе является «Json», разобранным в словарь, то есть каждое значение в серии данных это ещё один набор значений в виде словаря. 
Для группировки по районам нужно извлечь название района из этой структуры. Для этого создаю отдельную функцию. Функция извлечения района будет брать значения серии у “ObjectAddress”, находить в этом значении поле «District» и затем приводить это поле к списку и возвращать первое значение из этого списка, то есть будет вычленять первый найденный «District» из словаря «ObjectAddress».
После получения района для всех значений данных можно сгруппировать по нему и отсортировать по числу посетителей библиотек в порядке убывания.
Строю результирующую круговую диаграмму из 20-ти самых популярных районов. Для большей читаемости убираю районы из подписи на диаграмме, задав пустые «labels» ровно по числу районов, которых всего 20. Переношу все районы в легенду диаграммы. Легенду вывожу справа от диаграммы, задав «bbox_to_anchor» соответствующим образом, сдвигаю по оси х, по оси у = 1 – прижатие к верхней границе, также задаю не нулевую ширину, чтобы легенда графика корректно отобразилась. После этого сохраняю график в файл для дальнейшей вставки в отчёт.
Чтобы сформировать отчёт, задаю шрифт, холст с размерами, наношу на холст данные по читателям библиотек и также номер страницы – второй, потому что  будет титульная страница с номером 1.
Вставляю нашу круговую диаграмму через отдельное изображение из того файла, в который была сохранена круговая диаграмма.
Вывожу информацию по самому популярному району, это первый район в отсортированном списке, его нужно отобразить, и вывожу число посетителей по первому кортежу.
В завершении объединяю сгенерированный отчёт и титульную страницу через «pdffilemanager». В итоговом Pdf-документе присутствуют все данные, которые туда были добавлены и которые необходимо было отобразить, а также ответ по суммарной посещаемости библиотек в самом популярном районе Москвы: 765402 человека.                                        
Код и пояснения к нему можно найти в файле: «Задание 13. Сборка PDF документа.ipynb». 


Задание 14. Геральдические символы Москвы
Сгенерируйте PDF документ из списка флагов и гербов районов Москвы:
https://video.ittensive.com/python-advanced/data-102743-2019-11-13.utf.csv
На каждой странице документа выведите название геральдического символа (Name), его описание (Description) и его изображение (Picture).
Для показа изображений используйте адрес https://op.mos.ru/MEDIA/showFile?id=XXX
где XXX - это значение поля Picture в наборе данных. 
Например:
https://op.mos.ru/MEDIA/showFile?id=8466da35-6801-41a9-a71e-04b60408accb
В случае возникновения проблем с загрузкой изображений с op.mos.ru можно добавить в код настройку для форсирования использования дополнительных видов шифрования в протоколе SSL/TLS.
requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'

Если на каждой странице PDF документа выводить по одному геральдическому символу, то сколько получится страниц?

Решение
Подключаю все необходимые библиотеки и загружаю данные.
Формирую html-документ для отчёта, добавляю в документ постраничный вывод данных.
Вывожу заголовки одного уровня и задаю стили для всех кроме первого.
Перебираю в цикле набор данных через «data.iterrows()». Вставляю разрыв страницы после первого заголовка, таким образом получаю каждый геральдический символ с названием на отдельной странице.
После вывода заголовка, обеспечивающего разрыв страницы, вывожу изображение геральдического символа в увеличенном виде, используя атрибут «style». Источник изображения «src» задаю через ссылку в нужном формате. К исходному формату ссылки добавляю значение поля «Picture» из кортежа данных. Вывожу описание символа, с помощью «font-size» делаю шрифт крупнее. Вывожу свойство «Description» кортежа.
Конфигурация «Pdfkit» происходит стандартным образом, указанием пути до бинарного файла. Задаю стандартные настройки, размер страницы и пагина́цию (порядковую нумерацию страниц). Генерирую из строки html-файл, сохраняю его в файл «heraldic.pdf», в котором 42 страницы.
Код и пояснения к нему можно найти в файле: «Задание 14. Геральдические символы Москвы.ipynb». 


Задание 15. Многостраничный отчёт
Используя данные по активностям в парках Москвы
https://video.ittensive.com/python-advanced/data-107235-2019-12-02.utf.json
создайте PDF отчёт, в котором выведите:
1. Диаграмму распределения числа активностей по паркам, топ 10 самых активных
2. Таблицу активностей по всем паркам в виде Активность-Расписание-Парк
Сколько активностей Тайцзицюань есть в парках Москвы?

Решение
Подключаю все необходимые библиотеки и загружаю данные. 
После просмотра данных формирую набор данных только из колонок «CourseName», «CursesTimetable», «NameOfPark». 
Извлекаю название парка из комбинированной серии с помощью лямбда функции, нужно получить значение “value” у словаря. После переименовываю колонки для отчёта и нахожу активность «Тайцзицюань» и вывожу число записей с этой активностью. Такая активность всего одна, всего одна запись есть.
Формирую диаграмму активности по паркам. Создаю для этого холст, группирую данные по парку, сортирую в порядке убывания, чтобы взять первые 20 значений, самые активные парки и нанести их на круговую диаграмму.
Для вывода изображения в отчёт использую объект «BytesIO» для временного хранения бинарных данных изображения и его дальнейшие преобразование в b64 формат (будет указатель в памяти, который используется для хранения данных изображений).
После того, как изображения по этому указателю сохранено можно преобразовать бинарные данные к base64 формату.
«Pandas» по умолчанию ограничивает длину данных в ячейках, если вывести все данные из набора данных в html-код, они обрежутся, останется 50 символов, а необходимо больше, поэтому понадобится задать настройку «.set_option» в библиотеке «Pandas» - «.max_colwidth» для вывода полного расписания Активности в парке в html-таблице.
Формирую html-отчёт. Генерирую «Pdf» из html-строки, используя «Pdfkit», получаю итоговый отчёт со всеми активностями и со всеми расписаниями.
Код и пояснения к нему можно найти в файле: «Задание 15. Многостраничный отчёт.ipynb».


Задание 16. Автоматические отчёты
Соберите отчёт по результатам ЕГЭ в 2018-2019 году, используя данные
https://video.ittensive.com/python-advanced/data-9722-2019-10-14.utf.csv
и отправьте его в HTML формате по адресу support@ittensive.com, используя только Python.

В отчёте должно быть:
• общее число отличников (учеников, получивших более 220 баллов по ЕГЭ в Москве),
• распределение отличников по округам Москвы,
• название школы с лучшими результатами по ЕГЭ в Москве.
Диаграмма распределения должна быть вставлена в HTML через data:URI формат (в base64-кодировке).
Дополнительно: приложите к отчёту PDF документ того же содержания (дублирующий письмо).
Сколько учеников набрало 220 и больше баллов по ЕГЭ в Москве в 2018-2019 году? 

Решение
Подключаю все необходимые библиотеки и загружаю данные. Сначала выделяю из данных результаты только 2018, 2019 года, нужно сделать все преобразования с данными, затем вставить результаты в отчёт, график и html-документ.
Нужно найти лучшую школу по результатам ЕГЭ. Для этого можно взять первое из сортированных значений и сортировку выполнить по “Passes_Over_220”, группирую по административному округу, предварительно убираю из административного округа все слова, кроме первого для того, чтобы подписи данных были короче и красивее.
Сортировка по административным округам выполнялась, чтобы 2 самых маленьких значения, самых малочисленных округа на графике вынести из графика, чтобы подписи поместились на сектор, соответствующий этим районам. Произвожу подсчёт общего числа отличников, создаю холст, на котором создаю список секторов и меру их выноса из основной диаграммы. Для первого и для второго сектора по порядку выношу, видно будет, что эти округа самые малочисленные. 
Создаю круговую диаграмму по округам, в подписи передаю пустой набор данных по числу округов, вывожу названия, автоподписи, сформированные из доли точного значения отличников по округам и список для выноса секторов, какие будут выноситься, какие нет.
Справа вывожу легенду с подписями округов. После этого создаю объект в памяти для отображения и сохранения изображения. Для вставки изображения в отчёт преобразую его в base64 кодировку.
Для корректного вывода длинного названия школы задаю настойку «Pandas» по длине значения в колонке.
Формирую html-отчёт со всеми данными. Создаю кодировку, вставляю суммарное значение числа отличников, распределение по округам и название лучшей школы.
Формируем через html Pdf-отчёт через «Pdfkit». Задаю настройки, размер страницы, вывод номера страницы на каждой странице и вызываю генерацию pdf-документа из строки, сохраняю его в файл «ege.best.pdf».
Оформляю отправку письма с этими отчётами. Создаю объект «MIMEMultipart» и создаю поля «from», «subject», «content-type». В тело письма прикрепляю html-документ и после вкладываю pdf-отчёт. Подключаюсь к почтовому серверу, отправляю письмо на адрес эл. адрес, указываю логин пользователя почтового ящика, адрес почтового сервера smtp (Yandex, Google, Mail и.т.д). 
Письмо дошло в нужном виде.
Код и пояснения к нему можно найти в файле: «Задание 16. Автоматические отчёты.ipynb».
